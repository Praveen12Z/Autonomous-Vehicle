{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Resize\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import fiftyone\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train' if necessary\n",
      "Downloading annotations to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/tmp-download/annotations_trainval2017.zip'\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|    1.9Gb/1.9Gb [16.5s elapsed, 0s remaining, 114.7Mb/s]      \n",
      "Extracting annotations to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/raw/instances_train2017.json'\n",
      "Downloading 84853 images\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84853/84853 [1.0h elapsed, 0s remaining, 20.2 images/s]      \n",
      "Writing annotations for 84853 downloaded samples to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train/labels.json'\n",
      "Downloading split 'validation' to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation' if necessary\n",
      "Found annotations at '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/raw/instances_val2017.json'\n",
      "Downloading 3574 images\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3574/3574 [2.7m elapsed, 0s remaining, 20.1 images/s]      \n",
      "Writing annotations for 3574 downloaded samples to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation/labels.json'\n",
      "Dataset info written to '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/info.json'\n",
      "Loading 'coco-2017' split 'train'\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84853/84853 [5.8m elapsed, 0s remaining, 241.2 samples/s]      \n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3574/3574 [13.9s elapsed, 0s remaining, 278.7 samples/s]      \n",
      "Dataset 'coco-2017-train-validation' created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset_train.export(\\n    export_dir=\"/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train\",\\n    dataset_type=fiftyone.types.COCODetectionDataset,\\n    label_field=\"ground_truth\"\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\"person\",\"bicycle\",\"car\",\"motorcycle\", \"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\"]\n",
    "\n",
    "# Load Training dataset\n",
    "dataset = fiftyone.zoo.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    splits=[\"train\",'validation'],\n",
    "    label_types=\"detections\",\n",
    "    only_matching=True,\n",
    "    classes=categories,\n",
    "    dataset_dir = './dataset'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Data into YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pathes\n",
    "input_path = './dataset'\n",
    "\n",
    "from ultralytics.data.converter import convert_coco\n",
    "\n",
    "#train yolo format\n",
    "convert_coco(labels_dir='/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train',save_dir = './converter_result')\n",
    "shutil.move('./converter_result/labels/labels', os.path.join(input_path,'train/labels'))\n",
    "shutil.rmtree('./converter_result')\n",
    "os.rename(os.path.join(input_path,'train/data'), os.path.join(input_path,'train/images'))\n",
    "\n",
    "\n",
    "#validation yolo format\n",
    "convert_coco(labels_dir='/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation',save_dir = './converter_result')\n",
    "shutil.move('./converter_result/labels/labels', os.path.join(input_path,'validation/labels'))\n",
    "shutil.rmtree('./converter_result')\n",
    "os.rename(os.path.join(input_path,'validation/data'), os.path.join(input_path,'validation/images'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "# Read Json file\n",
    "def read_json_file(path):\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "# Load Json anotation: dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
    "train_labels = read_json_file(os.path.join(input_path,'train/labels.json'))\n",
    "val_labels = read_json_file(os.path.join(input_path,'validation/labels.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo Benchmark:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('./model/yolov8n.pt') \n",
    "metrics = model.val(data='coco.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.29 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.9.12 torch-2.2.1 CPU (Intel Core(TM) i9-9880H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=./model/yolov8n.pt, data=coco.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train/labels... 84853 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84853/84853 [00:42<00:00, 2014.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation/labels.cache... 3574 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3574/3574 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('./model/yolov8n.pt')  \n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='coco.yaml', epochs=1, imgsz=640)\n",
    "#results = model.train(data='coco.yaml', epochs=100, imgsz=640, device=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.9.12 torch-2.2.1 CPU (Intel Core(TM) i9-9880H 2.30GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation/labels... 3574 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3574/3574 [00:01<00:00, 1869.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset/validation/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [03:48<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3574      29168      0.607      0.421      0.462      0.324\n",
      "                person       3574      10777      0.759      0.673      0.747      0.516\n",
      "               bicycle       3574        314      0.687      0.392      0.458      0.265\n",
      "                   car       3574       1918      0.649      0.516      0.563      0.364\n",
      "            motorcycle       3574        367      0.717       0.58      0.659      0.415\n",
      "              airplane       3574        143      0.833      0.766      0.835      0.654\n",
      "                   bus       3574        283       0.74      0.643      0.739      0.621\n",
      "                 train       3574        190      0.794      0.768      0.836      0.647\n",
      "                 truck       3574        414      0.554      0.402      0.437      0.295\n",
      "                  boat       3574        424       0.58        0.3      0.379      0.211\n",
      "         traffic light       3574        634      0.655      0.347      0.414      0.213\n",
      "          fire hydrant       3574        101      0.869      0.703      0.789      0.625\n",
      "             stop sign       3574         75      0.727       0.64      0.703      0.638\n",
      "         parking meter       3574         60      0.644        0.5      0.562      0.444\n",
      "                 bench       3574        411      0.598      0.261      0.303      0.197\n",
      "                  bird       3574        427      0.698      0.358      0.435      0.285\n",
      "                   cat       3574        202       0.81      0.824       0.87      0.664\n",
      "                   dog       3574        218       0.68      0.701      0.736      0.597\n",
      "                 horse       3574        272      0.791      0.654      0.724      0.545\n",
      "                 sheep       3574        152      0.564      0.526      0.543       0.35\n",
      "                   cow       3574        133      0.569      0.546      0.569      0.402\n",
      "              elephant       3574         93      0.562      0.828      0.749      0.556\n",
      "                  bear       3574          6      0.469        0.5      0.508      0.463\n",
      "                 zebra       3574         18      0.709      0.833       0.85      0.725\n",
      "               giraffe       3574         41      0.776      0.756      0.809      0.586\n",
      "              backpack       3574        362      0.489      0.146       0.19     0.0956\n",
      "              umbrella       3574        369       0.62      0.507      0.544      0.358\n",
      "               handbag       3574        533      0.498      0.126      0.177     0.0883\n",
      "                   tie       3574        247      0.684      0.364      0.437      0.272\n",
      "              suitcase       3574        242      0.545       0.38       0.44      0.285\n",
      "               frisbee       3574        111      0.748      0.802      0.794      0.605\n",
      "                  skis       3574        241      0.609      0.323      0.365      0.187\n",
      "             snowboard       3574         68      0.483      0.324      0.381      0.265\n",
      "           sports ball       3574        252      0.723      0.444      0.479      0.331\n",
      "                  kite       3574        309      0.615      0.528      0.569      0.385\n",
      "          baseball bat       3574        132      0.591      0.424       0.44      0.238\n",
      "        baseball glove       3574        147      0.669      0.476      0.518      0.307\n",
      "            skateboard       3574        175      0.715      0.616      0.663      0.453\n",
      "             surfboard       3574        253      0.616      0.464      0.492      0.292\n",
      "         tennis racket       3574        224      0.704      0.616      0.671        0.4\n",
      "                bottle       3574        643      0.603      0.365      0.432       0.28\n",
      "            wine glass       3574        248      0.631      0.359      0.427       0.27\n",
      "                   cup       3574        587      0.603      0.472      0.523      0.365\n",
      "                  fork       3574        114      0.502      0.168      0.221      0.134\n",
      "                 knife       3574        157       0.42      0.134      0.138      0.074\n",
      "                 spoon       3574        133      0.403      0.113      0.142     0.0882\n",
      "                  bowl       3574        307      0.591      0.469      0.497      0.361\n",
      "                banana       3574        201      0.584      0.251      0.331      0.187\n",
      "                 apple       3574        105      0.548      0.229      0.265      0.193\n",
      "              sandwich       3574         74      0.444      0.297      0.215      0.123\n",
      "                orange       3574        112       0.43      0.263      0.274      0.207\n",
      "              broccoli       3574         22      0.483     0.0909      0.107     0.0844\n",
      "                carrot       3574         82      0.308     0.0816     0.0744     0.0402\n",
      "               hot dog       3574         99      0.741      0.404      0.462      0.313\n",
      "                 pizza       3574        192      0.588      0.505      0.548      0.394\n",
      "                 donut       3574        186      0.558      0.387      0.446      0.329\n",
      "                  cake       3574        200      0.657      0.374      0.484      0.319\n",
      "                 chair       3574       1434       0.55      0.302      0.356      0.213\n",
      "                 couch       3574        162      0.502       0.46      0.468      0.317\n",
      "          potted plant       3574        207      0.537      0.329       0.34      0.211\n",
      "                   bed       3574        105      0.506      0.457      0.483      0.333\n",
      "          dining table       3574        435      0.541      0.369      0.385      0.233\n",
      "                toilet       3574         39       0.61      0.564      0.624      0.498\n",
      "                    tv       3574        168      0.717      0.524      0.601      0.474\n",
      "                laptop       3574        144      0.592       0.59      0.594      0.472\n",
      "                 mouse       3574         29      0.487      0.586      0.681      0.515\n",
      "                remote       3574        243      0.449      0.228      0.282      0.162\n",
      "              keyboard       3574         53      0.575      0.566      0.568      0.423\n",
      "            cell phone       3574        218       0.56      0.321      0.381      0.257\n",
      "             microwave       3574         15      0.404        0.2      0.259      0.201\n",
      "                  oven       3574         62      0.686      0.306      0.389      0.203\n",
      "               toaster       3574          1          1          0          0          0\n",
      "                  sink       3574         50      0.543       0.38      0.371      0.229\n",
      "          refrigerator       3574         51      0.431      0.314      0.374      0.253\n",
      "                  book       3574        559      0.439     0.0923      0.159     0.0763\n",
      "                 clock       3574        138      0.694      0.507      0.593      0.394\n",
      "                  vase       3574        100      0.425       0.32       0.27      0.178\n",
      "              scissors       3574         18      0.529      0.167      0.185       0.16\n",
      "            teddy bear       3574         93      0.566      0.387      0.424      0.311\n",
      "            hair drier       3574          5          1          0    0.00724    0.00651\n",
      "            toothbrush       3574         39       0.39      0.154      0.233      0.161\n",
      "Speed: 1.1ms preprocess, 53.8ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('./model/yolov8n.pt') \n",
    "metrics = model.val(data='coco.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "\n",
    "\n",
    "\n",
    "# Copy files from one folder to another\n",
    "def copy_files_to_folder(source, destination):\n",
    "    count = 0\n",
    "    file_names = []\n",
    "    for filename in os.listdir(source):\n",
    "        source_file = os.path.join(source,filename)\n",
    "        destination_file = f\"{destination}/img{count}.jpg\"\n",
    "\n",
    "        try:\n",
    "            shutil.copy(source_file, destination_file)\n",
    "        except shutil.SameFileError:\n",
    "            print(\"Source and destination represents the same file.\")\n",
    "\n",
    "        file_names.append(filename)\n",
    "        count += 1\n",
    "\n",
    "    return file_names\n",
    "\n",
    "# Get image annotation\n",
    "def get_img_ann(image_id,labels_dict):\n",
    "    img_ann = []\n",
    "    isFound = False\n",
    "    for ann in labels_dict['annotations']:\n",
    "        if ann['image_id'] == image_id:\n",
    "            img_ann.append(ann)\n",
    "            isFound = True\n",
    "    if isFound:\n",
    "        return img_ann\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Get image\n",
    "def get_img(filename,labels_dict):\n",
    "\tfor img in labels_dict['images']:\n",
    "\t\tif img['file_name'] == filename:\n",
    "\t\t\treturn img\n",
    "\n",
    "\n",
    "# Create Labels in YOLO format\n",
    "          \n",
    "def create_labels_yolo(file_names,labels_dict,output_path):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for filename in file_names:\n",
    "        # Extracting image \n",
    "        img = get_img(filename,labels_dict)\n",
    "        img_id = img['id']\n",
    "        img_w = img['width']\n",
    "        img_h = img['height']\n",
    "\n",
    "        # Get Annotations for this image\n",
    "        img_ann = get_img_ann(img_id,labels_dict)\n",
    "\n",
    "        if img_ann:\n",
    "            # Opening file for current image\n",
    "            file_object = open(f\"{output_path}/labels/img{count}.txt\", \"a\")\n",
    "\n",
    "            for ann in img_ann:\n",
    "                current_category = ann['category_id'] - 1 # As yolo format labels start from 0 \n",
    "                current_bbox = ann['bbox']\n",
    "                x = current_bbox[0]\n",
    "                y = current_bbox[1]\n",
    "                w = current_bbox[2]\n",
    "                h = current_bbox[3]\n",
    "\n",
    "                # Finding midpoints\n",
    "                x_centre = (x + (x+w))/2\n",
    "                y_centre = (y + (y+h))/2\n",
    "\n",
    "                # Normalization\n",
    "                x_centre = x_centre / img_w\n",
    "                y_centre = y_centre / img_h\n",
    "                w = w / img_w\n",
    "                h = h / img_h\n",
    "\n",
    "                # Limiting upto fix number of decimal places\n",
    "                x_centre = format(x_centre, '.6f')\n",
    "                y_centre = format(y_centre, '.6f')\n",
    "                w = format(w, '.6f')\n",
    "                h = format(h, '.6f')\n",
    "                    \n",
    "                # Writing current object \n",
    "                file_object.write(f\"{current_category} {x_centre} {y_centre} {w} {h}\\n\")\n",
    "\n",
    "            file_object.close()\n",
    "            count += 1  # This should be outside the if img_ann block.\n",
    "\n",
    "\n",
    "# Torch utilities:\n",
    "\n",
    "def get_resize_size(transform):\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, Resize):\n",
    "            return t.size\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pathes\n",
    "input_path = '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset'\n",
    "output_path = '/Users/daniil.yefimov/Desktop/GitHub/Autonomous-Vehicle/dataset_yolo'\n",
    "\n",
    "\n",
    "\n",
    "# Copy Image files to the correct folder\n",
    "train_image_names = copy_files_to_folder(os.path.join(input_path,'train/data'),os.path.join(output_path,'train/images'))\n",
    "val_image_names = copy_files_to_folder(os.path.join(input_path,'validation/data'),os.path.join(output_path,'validation/images'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Labels in Yolo format\n",
    "create_labels_yolo(train_image_names,train_labels,os.path.join(output_path,'train'))\n",
    "create_labels_yolo(val_image_names,val_labels,os.path.join(output_path,'validation'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
